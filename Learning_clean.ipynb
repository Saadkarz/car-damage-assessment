{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04ae5042",
   "metadata": {},
   "source": [
    "# Car Damage Assessment - Clean Working Version\n",
    "\n",
    "This notebook contains only the essential, verified working components for car damage assessment with GPU support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289b69d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete GPU Setup & Verification for Car Damage Assessment\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "from transformers import AutoProcessor\n",
    "from unsloth import FastVisionModel\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "\n",
    "print(\"üöÄ === COMPLETE GPU SETUP & VERIFICATION ===\")\n",
    "\n",
    "# Import verification\n",
    "print(\"‚úÖ All required packages imported\")\n",
    "\n",
    "print(\"\\nüß™ GPU Status Check:\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"‚úÖ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    print(f\"‚úÖ CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA not available!\")\n",
    "    exit()\n",
    "\n",
    "# Test basic GPU operations\n",
    "print(\"\\nüß™ Testing basic GPU operations...\")\n",
    "try:\n",
    "    x = torch.randn(1000, 1000, device='cuda')\n",
    "    y = torch.randn(1000, 1000, device='cuda')\n",
    "    z = torch.mm(x, y)\n",
    "    print(\"‚úÖ Basic GPU operations working!\")\n",
    "    del x, y, z\n",
    "    torch.cuda.empty_cache()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå GPU operations failed: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Load and verify model\n",
    "print(\"\\nüì¶ Loading model and dataset...\")\n",
    "try:\n",
    "    # Load the trained model\n",
    "    model, tokenizer = FastVisionModel.from_pretrained(\n",
    "        model_name=\"Kakyoin03/car-damage-assessment-llama-vision\",\n",
    "        load_in_4bit=True,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    FastVisionModel.for_inference(model)\n",
    "    print(\"‚úÖ Model loaded successfully!\")\n",
    "    \n",
    "    # Load test dataset\n",
    "    dataset = load_dataset(\"Kakyoin03/car_damage_detection_dataset\", split=\"train\")\n",
    "    print(f\"‚úÖ Dataset loaded: {len(dataset)} samples\")\n",
    "    \n",
    "    print(f\"‚úÖ Model type: {type(model)}\")\n",
    "    print(f\"‚úÖ Model device: {next(model.parameters()).device}\")\n",
    "    print(f\"‚úÖ Tokenizer loaded: {type(tokenizer)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Model/Dataset loading failed: {e}\")\n",
    "    print(\"üîÑ This is expected if models aren't deployed yet\")\n",
    "\n",
    "print(\"\\nüéâ EVERYTHING READY FOR GPU INFERENCE!\")\n",
    "print(\"‚úÖ You can now run the inference code below.\")\n",
    "print(\"=\" * 56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a189d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Working Inference Function\n",
    "def analyze_car_damage_simple(image_path_or_pil, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Simple car damage analysis function that works reliably\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Clear GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Load and prepare image\n",
    "        if isinstance(image_path_or_pil, str):\n",
    "            image = Image.open(image_path_or_pil).convert(\"RGB\")\n",
    "        else:\n",
    "            image = image_path_or_pil.convert(\"RGB\")\n",
    "        \n",
    "        # Resize if too large\n",
    "        max_size = (1024, 1024)\n",
    "        if image.size[0] > max_size[0] or image.size[1] > max_size[1]:\n",
    "            image.thumbnail(max_size, Image.Resampling.LANCZOS)\n",
    "        \n",
    "        # Instruction\n",
    "        instruction = \"\"\"You are a car damage assessment expert. \n",
    "        Analyze this image and describe:\n",
    "        1. What parts of the car are damaged\n",
    "        2. Type of damage (scratch, dent, crack, etc.)\n",
    "        3. Severity level (minor, moderate, major)\n",
    "        Be concise and accurate.\"\"\"\n",
    "        \n",
    "        # Prepare input\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},\n",
    "                {\"type\": \"text\", \"text\": instruction}\n",
    "            ]}\n",
    "        ]\n",
    "        \n",
    "        # Tokenize\n",
    "        input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "        inputs = tokenizer(\n",
    "            image,\n",
    "            input_text,\n",
    "            add_special_tokens=False,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=150,\n",
    "                use_cache=False,\n",
    "                temperature=0.8,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.pad_token_id if hasattr(tokenizer, 'pad_token_id') else tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract response\n",
    "        if \"assistant\" in response:\n",
    "            response = response.split(\"assistant\")[-1].strip()\n",
    "        \n",
    "        # Clean up\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        torch.cuda.empty_cache()\n",
    "        return f\"Analysis failed: {str(e)}\"\n",
    "\n",
    "print(\"‚úÖ Simple inference function defined!\")\n",
    "print(\"üìù Usage: result = analyze_car_damage_simple(image, model, tokenizer)\")\n",
    "\n",
    "# Test if model is available\n",
    "try:\n",
    "    if 'model' in globals() and 'tokenizer' in globals():\n",
    "        print(\"‚úÖ Model and tokenizer are ready for testing!\")\n",
    "        \n",
    "        # Test with first dataset image if available\n",
    "        if 'dataset' in globals() and len(dataset) > 0:\n",
    "            print(\"\\nüß™ Testing with sample image...\")\n",
    "            test_image = dataset[0][\"image\"]\n",
    "            result = analyze_car_damage_simple(test_image, model, tokenizer)\n",
    "            print(f\"üìä Sample result: {result[:100]}...\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è Model not loaded yet - run the setup cell first\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ÑπÔ∏è Test skipped: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfba6be",
   "metadata": {},
   "source": [
    "## Usage Examples\n",
    "\n",
    "Once the model is loaded, you can use the inference function like this:\n",
    "\n",
    "```python\n",
    "# Analyze an image from file\n",
    "result = analyze_car_damage_simple(\"path/to/image.jpg\", model, tokenizer)\n",
    "print(result)\n",
    "\n",
    "# Analyze an image from dataset\n",
    "test_image = dataset[0][\"image\"]\n",
    "result = analyze_car_damage_simple(test_image, model, tokenizer)\n",
    "print(result)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
